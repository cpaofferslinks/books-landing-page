<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Build a Large Language Model (From Scratch) - Books4Everyone</title>
    <meta name="description" content="Download Build a Large Language Model (From Scratch) by Sebastian Raschka for free. Artificial Intelligence book available in PDF format.">
    
    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
    
    <!-- CSS -->
    <link rel="stylesheet" href="../css/modern-style.css">
    <link rel="stylesheet" href="../css/modern-books.css">
    <link rel="stylesheet" href="../css/modern-book-detail.css">
</head>
<body>
    <!-- Header -->
    <header class="header">
        <div class="container">
            <nav class="navbar">
                <a href="../index.html" class="logo">
                    <span class="logo-icon">&#128218;</span>
                    Books<span>4Everyone</span>
                </a>
                <div class="nav-menu">
                    <a href="../index.html" class="nav-link">Home</a>
                    <a href="../categories.html" class="nav-link">Categories</a>
                    <a href="#about" class="nav-link">About</a>
                </div>
            </nav>
        </div>
    </header>

    <!-- Book Detail Section -->
    <section class="book-detail-section">
        <div class="container">
            <div class="breadcrumb">
                <a href="../index.html">Home</a> > 
                <a href="../categories.html">Categories</a> > 
                <a href="../categories/artificial-intelligence.html">Artificial Intelligence</a> > 
                <span>Build a Large Language Model (From Scratch)</span>
            </div>
            
            <div class="book-detail-container">
                <div class="book-cover">
                    <img src="https://itbooks.ir/assets/images/books/artificial-intelligence/build-a-large-language-model-from-scratch120-150.webp" alt="Build a Large Language Model (From Scratch)" class="book-image">
                    <div class="book-actions">
                        <a href="https://itbooks.ir/assets/files/books/artificial-intelligence/build-a-large-language-model-from-scratch.pdf" class="btn btn-download-large" data-filename="build-a-large-language-model-from-scratch.pdf">
                            <span class="download-icon">ðŸ“¥</span>
                            Download PDF
                        </a>
                    </div>
                </div>
                
                <div class="book-info">
                    <h1 class="book-title">Build a Large Language Model (From Scratch)</h1>
                    <p class="book-author">by Sebastian Raschka</p>
                    <p class="book-category">Artificial Intelligence</p>
                    
                    <div class="book-specifications">
                        <h3>Book Details</h3>
                        <div class="specs-grid">
                            <div class="spec-item">
                                <h4>Book Title</h4>
                                <p>Build a Large Language Model (From Scratch)</p>
                            </div>
                            <div class="spec-item">
                                <h4>Author</h4>
                                <p>Sebastian Raschka</p>
                            </div>
                            <div class="spec-item">
                                <h4>Publisher</h4>
                                <p>Manning Publications Co</p>
                            </div>
                            <div class="spec-item">
                                <h4>Publication Date</h4>
                                <p>September 2024</p>
                            </div>
                            <div class="spec-item">
                                <h4>ISBN</h4>
                                <p>9781633437166</p>
                            </div>
                            <div class="spec-item">
                                <h4>Number of Pages</h4>
                                <p>593</p>
                            </div>
                            <div class="spec-item">
                                <h4>Language</h4>
                                <p>English</p>
                            </div>
                            <div class="spec-item">
                                <h4>Format</h4>
                                <p>PDF</p>
                            </div>
                            <div class="spec-item">
                                <h4>File Size</h4>
                                <p>7MB</p>
                            </div>
                            <div class="spec-item">
                                <h4>Subject</h4>
                                <p>Artificial Intelligence > Deep Learning > Large Language
                  Models (LLMs)</p>
                            </div>
                        </div>
                    </div>
                    
                    <div class="table-of-contents"><h3>Table of Contents</h3><div class="toc-list"><ul><li>Contents</li><li>Build a Large Language Model (From Scratch)</li><li>Preface</li><li>Acknowledgments</li><li>About This Book</li><li>About the Author</li><li>About the Cover Illustration</li><li>1 Understanding Large Language Models</li><li>1.1 What is an LLM?</li><li>1.2 Applications of LLMs</li><li>1.3 Stages of building and using LLMs</li><li>1.4 Introducing the transformer architecture</li><li>1.5 Utilizing large datasets</li><li>1.6 A closer look at the GPT architecture</li><li>1.7 Building a large language model</li><li>2 Working with Text Data</li><li>2.1 Understanding word embeddings</li><li>2.2 Tokenizing text</li><li>2.3 Converting tokens into token IDs</li><li>2.4 Adding special context tokens</li><li>2.5 Byte pair encoding</li><li>2.6 Data sampling with a sliding window</li><li>2.7 Creating token embeddings</li><li>2.8 Encoding word positions</li><li>3 Coding Attention Mechanisms</li><li>3.1 The problem with modeling long sequences</li><li>3.2 Capturing data dependencies with attention mechanisms</li><li>3.3 Attending to different parts of the input with
                  self-attention</li><li>3.4 Implementing self-attention with trainable weights</li><li>3.5 Hiding future words with causal attention</li><li>3.6 Extending single-head attention to multi-head attention</li><li>4 Implementing a GPT Model from Scratch to Generate
                    Text</li><li>4.1 Coding an LLM architecture</li><li>4.2 Normalizing activations with layer normalization</li><li>4.3 Implementing a feed-forward network with GELU activations</li><li>4.4 Adding shortcut connections</li><li>4.5 Connecting attention and linear layers in a transformer
                  block</li><li>4.6 Coding the GPT model</li><li>4.7 Generating text</li><li>5 Pretraining on Unlabeled Data</li><li>5.1 Evaluating generative text models</li><li>5.2 Training an LLM</li><li>5.3 Decoding strategies to control randomness</li><li>5.4 Loading and saving model weights in PyTorch</li><li>5.5 Loading pretrained weights from OpenAI</li><li>6 Fine-Tuning for Classification</li><li>6.1 Different categories of fine-tuning</li><li>6.2 Preparing the dataset</li><li>6.3 Creating data loaders</li><li>6.4 Initializing a model with pretrained weights</li><li>6.5 Adding a classification head</li><li>6.6 Calculating the classification loss and accuracy</li><li>6.7 Fine-tuning the model on supervised data</li><li>6.8 Using the LLM as a spam classifier</li><li>7 Fine-Tuning to Follow Instructions</li><li>7.1 Introduction to instruction fine-tuning</li><li>7.2 Preparing a dataset for supervised instruction fine-tuning</li><li>7.3 Organizing data into training batches</li><li>7.4 Creating data loaders for an instruction dataset</li><li>7.5 Loading a pretrained LLM</li><li>7.6 Fine-tuning the LLM on instruction data</li><li>7.7 Extracting and saving responses</li><li>7.8 Evaluating the fine-tuned LLM</li><li>7.9 Conclusions</li><li>Appendix A: Introduction to PyTorch</li><li>A.1 What is PyTorch?</li><li>A.2 Understanding tensors</li><li>A.3 Seeing models as computation graphs</li><li>A.4 Automatic differentiation made easy</li><li>A.5 Implementing multilayer neural networks</li><li>A.6 Setting up efficient data loaders</li><li>A.7 A typical training loop</li><li>A.8 Saving and loading models</li><li>A.9 Optimizing training performance with GPUs</li><li>Appendix B: References and Further Reading</li><li>Appendix C: Exercise Solutions</li><li>Appendix D: Adding Bells and Whistles to the Training
                    Loop</li><li>D.1 Learning rate warmup</li><li>D.2 Cosine decay</li><li>D.3 Gradient clipping</li><li>D.4 The modified training function</li><li>Appendix E: Parameter-Efficient Fine-Tuning with
                    LoRA</li><li>E.1 Introduction to LoRA</li><li>E.2 Preparing the dataset</li><li>E.3 Initializing the model</li><li>E.4 Parameter-efficient fine-tuning with LoRA</li></ul></div></div>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div>
                    <h3>Books4Everyone</h3>
                    <p>Your gateway to free programming and technology books.</p>
                </div>
                <div>
                    <h3>Quick Links</h3>
                    <p><a href="../categories.html">Browse Categories</a></p>
                    <p><a href="../index.html">Home</a></p>
                </div>
            </div>
            <div class="footer-bottom">
                <p>&copy; 2024 Books4Everyone. All books are free to download and share.</p>
            </div>
        </div>
    </footer>
    
    <!-- JavaScript for handling downloads -->
    <script src="../js/download-handler.js"></script>
</body>
</html>